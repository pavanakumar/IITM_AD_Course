!        Generated by TAPENADE     (INRIA, Ecuador team)
!  Tapenade 3.16 (develop) - 15 Jan 2024 09:25
!
!  Differentiation of run_network in reverse (adjoint) mode:
!   gradient     of useful results: xout3
!   with respect to varying inputs: xout3 params
!   RW status of diff variables: xout3:in-out params:out
SUBROUTINE RUN_NETWORK_B(nnodes, nedges, edgelist, params, paramsb, &
& edgefeature1, x, xout3, xout3b) BIND(c, name='run_network_b')
  USE ISO_C_BINDING
  IMPLICIT NONE
  INTEGER(c_int32_t), INTENT(IN) :: nnodes
  INTEGER(c_int32_t), INTENT(IN) :: nedges
  INTEGER(c_int32_t), DIMENSION(2, nedges), INTENT(IN) :: edgelist
  REAL(c_float), DIMENSION(461), INTENT(IN) :: params
  REAL(c_float), DIMENSION(461) :: paramsb
  REAL(c_float), DIMENSION(1, nedges), INTENT(IN) :: edgefeature1
  REAL(c_float), DIMENSION(1, nnodes), INTENT(IN) :: x
  REAL(c_float), DIMENSION(1, nnodes) :: xout3
  REAL(c_float), DIMENSION(1, nnodes) :: xout3b
!$acc data copyin(edgelist(1:2, 1:nedges), params(1:461), EdgeFeature1(1:1, 1:nedges), x(1:1, 1:nnodes)) copyout(xout3(1:1, 1:nno
!des))
!$acc end data
  CALL ENCODER_2_B(nnodes, nedges, edgelist, params(1:461), paramsb(1:&
&            461), x, xout3, xout3b)
END SUBROUTINE RUN_NETWORK_B

!  Differentiation of encoder_2 in reverse (adjoint) mode:
!   gradient     of useful results: xout3
!   with respect to varying inputs: xout3 params
SUBROUTINE ENCODER_2_B(nnodes, nedges, edgelist, params, paramsb, x, &
& xout3, xout3b) BIND(c, name='Encoder_2_b')
  USE ISO_C_BINDING
  IMPLICIT NONE
  INTEGER(c_int32_t), INTENT(IN) :: nnodes
  INTEGER(c_int32_t), INTENT(IN) :: nedges
  INTEGER(c_int32_t), DIMENSION(2, nedges), INTENT(IN) :: edgelist
  REAL(c_float), DIMENSION(461), INTENT(IN) :: params
  REAL(c_float), DIMENSION(461) :: paramsb
  REAL(c_float), DIMENSION(1, nnodes), INTENT(IN) :: x
  REAL(c_float), DIMENSION(1, nnodes) :: xout3
  REAL(c_float), DIMENSION(1, nnodes) :: xout3b
  REAL(c_float) :: xin(1)
  REAL(c_float) :: xinb(1)
  REAL(c_float) :: x1(10)
  REAL(c_float) :: x1b(10)
  REAL(c_float) :: x1_noact(10)
  REAL(c_float) :: x1_noactb(10)
  REAL(c_float) :: x1_act(10)
  REAL(c_float) :: x1_actb(10)
  REAL(c_float) :: x2(20)
  REAL(c_float) :: x2b(20)
  REAL(c_float) :: x2_noact(20)
  REAL(c_float) :: x2_noactb(20)
  REAL(c_float) :: x2_act(20)
  REAL(c_float) :: x2_actb(20)
  REAL(c_float) :: x3(10)
  REAL(c_float) :: x3b(10)
  REAL(c_float) :: x3_noact(10)
  REAL(c_float) :: x3_noactb(10)
  REAL(c_float) :: x3_act(10)
  REAL(c_float) :: x3_actb(10)
  REAL(c_float) :: xout(1)
  REAL(c_float) :: xoutb(1)
  REAL(c_float) :: xout_noact(1)
  REAL(c_float) :: xout_noactb(1)
  REAL(c_float) :: xout_act(1)
  REAL(c_float) :: xout_actb(1)
  INTEGER(c_int32_t) :: i, i1, i2, iedge, inode
  paramsb = 0.0_4
  xoutb = 0.0_4
  x1b = 0.0_4
  x2b = 0.0_4
  x3b = 0.0_4
!#acc parallel loop gang vector present(edgelist, params, x, xout3) private(xin, x1, x2, x3, xout, x1_noact, x2_no
!act, x3_noact, xout_noact, x1_act, x2_act, x3_act, xout_act)
  DO inode=1,nnodes
!#acc loop seq
! Copy contents into temporary for MLP
    DO i=1,1
      xin(i+0) = x(i, inode)
    END DO
! Layer 1
    CALL DENSELAYER(1, 10, params(1), params(11), xin, x1_noact)
    CALL TANGENTHYPERBOLIC(10, x1_noact, x1_act)
!#acc loop seq
    DO i=1,10
      x1(i) = x1_act(i)
    END DO
! Layer 2
    CALL DENSELAYER(10, 20, params(21), params(221), x1, x2_noact)
    CALL TANGENTHYPERBOLIC(20, x2_noact, x2_act)
!#acc loop seq
    DO i=1,20
      x2(i) = x2_act(i)
    END DO
! Layer 3
    CALL DENSELAYER(20, 10, params(241), params(441), x2, x3_noact)
    CALL TANGENTHYPERBOLIC(10, x3_noact, x3_act)
!#acc loop seq
    DO i=1,10
      x3(i) = x3_act(i)
    END DO
!#acc loop seq
    DO i=1,1
      xoutb(i) = xoutb(i) + xout3b(i, inode)
      xout3b(i, inode) = 0.0_4
    END DO
    xout_actb = 0.0_4
!#acc loop seq
    DO i=1,1
      xout_actb(i) = xout_actb(i) + xoutb(i)
      xoutb(i) = 0.0_4
    END DO
    CALL IDENTITY_B(1, xout_noact, xout_noactb, xout_act, xout_actb)
    CALL DENSELAYER_B(10, 1, params(451), paramsb(451), params(461), &
&               paramsb(461), x3, x3b, xout_noact, xout_noactb)
    x3_actb = 0.0_4
!#acc loop seq
    DO i=1,10
      x3_actb(i) = x3_actb(i) + x3b(i)
      x3b(i) = 0.0_4
    END DO
    CALL TANGENTHYPERBOLIC_B(10, x3_noact, x3_noactb, x3_act, x3_actb)
    CALL DENSELAYER_B(20, 10, params(241), paramsb(241), params(441), &
&               paramsb(441), x2, x2b, x3_noact, x3_noactb)
    x2_actb = 0.0_4
!#acc loop seq
    DO i=1,20
      x2_actb(i) = x2_actb(i) + x2b(i)
      x2b(i) = 0.0_4
    END DO
    CALL TANGENTHYPERBOLIC_B(20, x2_noact, x2_noactb, x2_act, x2_actb)
    CALL DENSELAYER_B(10, 20, params(21), paramsb(21), params(221), &
&               paramsb(221), x1, x1b, x2_noact, x2_noactb)
    x1_actb = 0.0_4
!#acc loop seq
    DO i=1,10
      x1_actb(i) = x1_actb(i) + x1b(i)
      x1b(i) = 0.0_4
    END DO
    CALL TANGENTHYPERBOLIC_B(10, x1_noact, x1_noactb, x1_act, x1_actb)
    xinb = 0.0_4
    CALL DENSELAYER_B(1, 10, params(1), paramsb(1), params(11), paramsb(&
&               11), xin, xinb, x1_noact, x1_noactb)
  END DO
!#acc parallel loop collapse(2) present(xout3)
  DO inode=1,nnodes
    DO i=1,1,-1
      xout3b(i, inode) = 0.0_4
    END DO
  END DO
END SUBROUTINE ENCODER_2_B

!  Differentiation of tangenthyperbolic in reverse (adjoint) mode:
!   gradient     of useful results: xout
!   with respect to varying inputs: xin
!$acc routine seq
SUBROUTINE TANGENTHYPERBOLIC_B(n, xin, xinb, xout, xoutb) BIND(c)
  USE ISO_C_BINDING
  IMPLICIT NONE
  INTEGER(c_int32_t), INTENT(IN) :: n
  REAL(c_float), INTENT(IN) :: xin(n)
  REAL(c_float) :: xinb(n)
  REAL(c_float) :: xout(n)
  REAL(c_float) :: xoutb(n)
  INTRINSIC TANH
  xinb = 0.0_4
  xinb = (1.0-TANH(xin)**2)*xoutb
END SUBROUTINE TANGENTHYPERBOLIC_B

!  Differentiation of identity in reverse (adjoint) mode:
!   gradient     of useful results: xout
!   with respect to varying inputs: xin
!$acc routine seq
SUBROUTINE IDENTITY_B(n, xin, xinb, xout, xoutb) BIND(c)
  USE ISO_C_BINDING
  IMPLICIT NONE
  INTEGER(c_int32_t), INTENT(IN) :: n
  REAL(c_float), INTENT(IN) :: xin(n)
  REAL(c_float) :: xinb(n)
  REAL(c_float) :: xout(n)
  REAL(c_float) :: xoutb(n)
  xinb = 0.0_4
  xinb = xoutb
END SUBROUTINE IDENTITY_B

!  Differentiation of denselayer in reverse (adjoint) mode:
!   gradient     of useful results: w xout xin b
!   with respect to varying inputs: w xin b
!$acc routine seq
SUBROUTINE DENSELAYER_B(nin, nout, w, wb, b, bb, xin, xinb, xout, xoutb)&
&BIND(c)
  USE ISO_C_BINDING
  IMPLICIT NONE
  INTEGER(c_int32_t), INTENT(IN) :: nin
  INTEGER(c_int32_t), INTENT(IN) :: nout
  REAL(c_float), INTENT(IN) :: w(nin, nout)
  REAL(c_float) :: wb(nin, nout)
  REAL(c_float), INTENT(IN) :: b(nout)
  REAL(c_float) :: bb(nout)
  REAL(c_float), INTENT(IN) :: xin(nin)
  REAL(c_float) :: xinb(nin)
  REAL(c_float) :: xout(nout)
  REAL(c_float) :: xoutb(nout)
  INTEGER(c_int32_t) :: i, j
  bb = bb + xoutb
!#acc loop seq
  DO j=1,nout
    DO i=nin,1,-1
      wb(i, j) = wb(i, j) + xin(i)*xoutb(j)
      xinb(i) = xinb(i) + w(i, j)*xoutb(j)
    END DO
  END DO
END SUBROUTINE DENSELAYER_B

